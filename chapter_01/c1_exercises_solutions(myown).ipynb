{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: An Introduction 2 Edition\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 1.1: Self-Play***  Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution: ***  I think it will learn the same optimal policy as playing against a random opponent, because I think the final optimal policy is fixed, the optimal action at each state is fixed, if the estimations could converge, it will learn the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 1.2: Symmetries***  Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the learning process described above to take advantage of this? In what ways would this change improve the learning process? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution: *** First subquestion: when update one position's reward, its symmetry position should have the same reward value as itself. Second subquestion: it will accelerate the learning process, just fill up the corresponding position with the same number without updating the number with td method. Third subquestion: In that case, because the opponent did not take advantage of symmetries as before, so it will treat the \"same\" positions as two distict positions, and make suboptimal action selection, and then the agent itself will also be influenced -- learn a suboptimal policy. So in that case, it is not necessarily for the symmetrically equivalent positions to have the same value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 1.3: Greedy Play*** Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best. Might it learn to play better, or worse, than a nongreedy player? What problems might occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution: *** It would be worse undoubtedly if the learning player was greedy. The agent won't explore the environment enough so as to the estimations are bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 1.4: Learning from Exploration*** Suppose learning updates occurred after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a set of probabilities. What are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution: *** First, if we learn from exploratory moves, which is the normal case, we choose an exploratory action at epsilon probability and update the estimation each time step. In that case, we can get to know whether the exploratory action good or bad. But in the other case, we just choose the exploratory action without update the estimations, so whether it is good or bad, when choose a greedy action, it will be treated as a mediocre action, it will slow down the agent learning the environment. To conclude, the set of probabilities which do learn from exploratory moves might be better to learn and more wins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 1.5: Other Improvements*** Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem as posed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution: *** Emmmmmmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
