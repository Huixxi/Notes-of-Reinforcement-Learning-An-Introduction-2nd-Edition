{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: An Introduction 2 Edition\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文版"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 什么是强化学习？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从交互中学习是几乎所有学习和智能理论下的基础思想。  \n",
    "**强化学习**是学习做什么 -- 如何将场景映射到行为，以便最大化一个数值的奖励信号。  \n",
    "**试探搜索**和**延迟奖励**是强化学习的两个重要显著特征。  \n",
    "机器学习的三个范式：\n",
    "* 监督学习： 从有知识的外部监督员提供的一组带标签的例子的训练集中学习。\n",
    "* 无监督学习： 在无标签的数据中发现其隐藏的结构\n",
    "* 强化学习： 试着最大化一个奖励信号\n",
    "\n",
    "权衡“探索”和“开发”是仅出现在强化学习中众多挑战中的一个：\n",
    "* 探索：探索未知的行为----目的是将来能够做更好的行为选择\n",
    "* 开发：开发已知的行为----在已经历过的行为中选择可获得最大奖励的行为\n",
    "![](https://steemitimages.com/640x0/https://steemitimages.com/DQmXH5tjBiS41iNtcyvh7s7Rj5z3SqGkcwoaV2otRJNx3FT/Exploration_vs._Exploitation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 强化学习的要素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了“代理”和“环境”，强化学习还有其他四个组成要素：“策略”，“奖励信号”，“价值函数”，和一个可选的“模型”要素。  \n",
    "* **策略：** 定义了学习代理在给定时间点下的行为。是从感知到的环境的状态到这些状态下应采取的行为间的映射。它可以是简单的函数或是查找表，也可是一个需要额外计算的搜索过程。\n",
    "* **奖励信号：** 定义了强化学习问题中的目标。在每个时间点，环境会发送一个数值给代理，这个数值被称为“奖励”，它代表了对于代理来说什么是好的事件和坏的事件。\n",
    "* **价值函数：** 定义了从长远来看什么是好的。它是代理从当先状态开始，到未来可期望的全部奖励积累的总和。有效估计价值的方法是在几乎所有的强化学习算法中最重要的组成部分。\n",
    "* **模型：** 利用模型和计划的解决方法被称为“基于模型”的方法，相反仅依靠试探的方法被称为“无模型”方法。\n",
    "![](https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 局限性和应用范围"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**强化学习：**\n",
    "* 严重依赖“状态”的概念--作为“策略”和“价值函数”的输入，有同时为“模型”的输入和输出。我们可以简单的将“状态”理解成在特定时间向“代理”传递的一种“环境是怎么样的”信号。\n",
    "* 从与“环境”交互的过程中学习，并且要去估计价值函数。\n",
    "\n",
    "**进化方法：**\n",
    "* 这类方法比如“遗传算法”、“遗传编程”、“模拟退火”等应用多个静态策略，每个策略在较长时间内与环境的一个单独实例交互。获得最多奖励的政策，以及这些政策的随机变种，会被延续到下一代政策中，这个过程会重复。\n",
    "* 假如策略空间足够小或者可以被结构化，以至于好的策略非常常见或者容易被找到，或者有充足的时间用于策略搜索，那么进化方法是有效的。此外，进化方法在“学习代理”不能够感知它所处环境的完整“状态”的问题具有优势。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 一个拓展的例子：井字游戏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何从零开始搭建一个井字游戏机器人\n",
    "* **第一步：搭建一个环境（状态）--棋盘**  \n",
    "我们通常使用一个矩阵来表示一个棋盘。除此之外，你的环境（状态）至少应提供 状态本身，游戏规则（游戏结束的标志），该状态下的胜利者。\n",
    "* **第二步：搭建一个代理**  \n",
    "这个代理应该具备感知环境的能力，能够通过与环境交互去估计每个状态的价值函数，并且每个时间点都能够通过某种方法（我们使用TD-Error）去更新这个价值估计。此外，它还应能够运用策略去挑选行为。\n",
    "* **第三步：搭建一个第三方让游戏跑起来**  \n",
    "这个第三方充当一个裁判的角色，去管理游戏和代理玩家，让代理去和环境进行交互。\n",
    "* **第四步：搭建一个人类玩家游戏接口**\n",
    "* **第五步：通过自己和自己下棋的方法训练代理**\n",
    "\n",
    "### Temploral-Difference学习方法\n",
    "* $V(s) \\leftarrow V(s) + \\alpha [V(s^{\\prime}) - V(s)]$\n",
    "\n",
    "\n",
    "### 够了！给我看代码！\n",
    "* [官方的Python代码](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction/blob/master/chapter01/tic_tac_toe.py)  \n",
    "* [自己实现的C++版本]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
