{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: An Introduction 2nd Edition\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 1  --  Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Reinforcement Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning from interaction is a fundational idea underlying nearly all theories of learning and intelligence.  \n",
    "  \n",
    "**Reinforcement learning** is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal.  \n",
    "  \n",
    "These two characteristics—**trial-and-error search** and **delayed reward**—are the two most important distinguishing features of reinforcement learning.  \n",
    "  \n",
    "There are three machine learning paradigms:\n",
    "* Supervise Learning: Learning from a training set of labeled examples provided by a knowledgable external supervisor.\n",
    "* Unsupervise Learning: Finding structure hidden in collections of unlabeled data.\n",
    "* Reinforcement Learning: Trying to maximize a reward signal.\n",
    "\n",
    "Trade-off between **exploration** and **exploitation** is one of the challenges that arise only in reinforcement learning:\n",
    "* Exploration: explore new actions in order to make better action selections in the future\n",
    "* Exploitation: exploit actions what it has already experienced in order to obtain best reward\n",
    "![](https://steemitimages.com/640x0/https://steemitimages.com/DQmXH5tjBiS41iNtcyvh7s7Rj5z3SqGkcwoaV2otRJNx3FT/Exploration_vs._Exploitation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Elements of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the **agent** and the **environment**, there are four main subelements of a reinforcement learning system: a **policy**, a **reward signal**, a **value function**, and optionally, a **model** of the environment.  \n",
    "* **Policy:** Defines the learning agent's way of behaving at a given time. A mapping from perceived states of the environment to actions to be taken when in those states. It could be a simple function or lookup table, or a search process.  \n",
    "* **Reward Signal:** Defines the goal in a reinforcement learning problem. At each time step, the environment sends the agent a single number called reward, it represents what are the good and bad events for the agent.  \n",
    "* **Value Function:** Defines what is good in the long run. It is the total amount of reward an agent can expect to accumulate over the future, starting from that state. **The most important component of almost all reinforcement learning algorithms we consider is a method for efficiently estimating values.**  \n",
    "* **Model:** Methods for solving reinforcement learning problems that use models and planning are called **model-based** methods, as opposed to simpler **model-free** methods that are explicitly trial-and-error learners—viewed as almost the opposite of planning.  \n",
    "![](https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Limitations and Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reinforcement learning:** \n",
    "* Heavily relies on the concept of **state**—as input to the policy and value function, and as both input to and output from the model. Informally, we can think of the state as a signal conveying to the agent some sense of “how the environment is” at a particular time. \n",
    "* Learning from interacting with the environment and estimating value functions\n",
    "\n",
    "**Evolutionary methods:** \n",
    "* Such as genetic algorithms, genetic programming, simulated annealing, apply multiple static policies each interacting over an extended period of time with a separate instance of the environment. The policies that obtain the most reward, and random variations of them, are carried over to the next generation of policies, and the process repeats.  \n",
    "* If the space of policies is sufficiently small, or can be structured so that good policies are common or easy to find—or if a lot of time is available for the search—then evolutionary methods can be effective. In addition, evolutionary methods have advantages on problems in which the learning agent cannot sense the complete state of its environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. An Extended Example: Tic-Tac-Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to build a tic-tac-toe robot from scratch\n",
    "* **Step 1: Building an environment(state), the board**  \n",
    "Normally, we use a matrix to represent a board. Besides, your environment(state) should provide the state itself, game rules(gameover sign), the winner under the state at least.\n",
    "* **Step 2: Building an agent**  \n",
    "The agent shoud have the ability of sensing the environment, estimating the value function of each state through interacting with the environment, and update the estimations through some methods(we use TD-Error) at each time step. Besides, it could use policy to select an action.\n",
    "* **Step 3: Buliding a third party to run the game**  \n",
    "The third party serve as a judger to manage the game and agents, and let the agents interact with the environment.\n",
    "* **Step 4: Building a human player interface**  \n",
    "* **Step 5: Training the agent with self-play method**\n",
    "\n",
    "### Temploral-Difference Learning Method\n",
    "* $V(s) \\leftarrow V(s) + \\alpha [V(s^{\\prime}) - V(s)]$\n",
    "\n",
    "\n",
    "### Shut up! Show me the code!\n",
    "* [Offical Release Python Code](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction/blob/master/chapter01/tic_tac_toe.py)  \n",
    "* [C++ Version]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
