{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: An Introduction 2nd Edition\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Dynamic -- Dynamic Programming\n",
    "Classical DP algorithms are of limited utility in reinforcement learning both because of their:\n",
    "* assumption of a perfect model \n",
    "* great computational expense.  \n",
    "\n",
    "\n",
    "The key idea of DP, and of reinforcement learning generally, is the use of *value functions* to organize and structure the search for good *policies*.  \n",
    "DP algorithms are obtained by turning *Bellman equations* such as these into *assignments*, that is, into *update rules* for improving approximations of the desired value functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Policy Evalution(Prediction)\n",
    "Compute the *state-value* function $v_π$ for an arbitrary policy $π$.  \n",
    "* ***Iterative policy evaluation***  \n",
    "$$ \n",
    "\\begin{split}\n",
    "v_{k+1} & \\doteq E_{\\pi}[R_{t+1} + \\gamma v_k(S_{t+1}) | S_t=s]  \\\\\n",
    "        & = \\sum\\limits_{a}\\pi(a|s) \\sum\\limits_{s',r}p(s',r|s,a) [r + \\gamma v_k(s')]\n",
    "\\end{split}\n",
    "$$\n",
    "The sequence $\\{v_k\\}$ can be shown in general to converge to $v_{\\pi}$ as $k \\rightarrow \\infty$, and this kind of update is called ***expected update***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 4.1*** In Example 4.1, if $π$ is the equiprobable random policy, what is $q_π(11, down)$? What is $q_π(7, down)$?    \n",
    "***Solution:*** $$ q_π(11, down) = \\sum\\limits_{s', r}p(s', r|11, down)[r + \\sum\\limits_{a' \\in A(s')}\\pi(a'|s')q_{\\pi}(s',a')] = -1 $$\n",
    "$$ q_π(7, down) = \\sum\\limits_{s', r}p(s', r|7, down)[r + \\sum\\limits_{a' \\in A(11)}\\pi(a'|11)q_{\\pi}(11,a')] = -1 + \\sum\\limits_{a' \\in A(11)}\\pi(a'|11)q_{\\pi}(11,a') $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 4.2***  \n",
    "***Solution:*** \n",
    "$$\n",
    "\\begin{split}\n",
    "v_{\\pi}(15) & = \\sum\\limits_{a}\\pi(a|s) \\sum\\limits_{s', r}p(s', r|s, a)[r + v_{\\pi}(s')]  \\\\\n",
    "            & = \\sum\\limits_{a}\\pi(a|15) \\sum\\limits_{s'}[-1 + v_{\\pi}(s')]\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 4.3***  \n",
    "***Solution:*** \n",
    "$$\n",
    "\\begin{split}\n",
    "q_{k+1}(s, a) & = \\sum\\limits_{s', r}p(s', r|s, a)[r + \\gamma \\sum\\limits_{a' \\in A(s')}\\pi(a'|s')q_{\\pi}(s',a')]  \\\\\n",
    "            & = \\sum\\limits_{s', r}p(s', r|s, a)[r + \\gamma \\sum\\limits_{a' \\in A(s')}\\pi(a'|s')q_{k}(s',a')]\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Policy Improvement\n",
    "The process of making a new policy that improves on an original policy, by making it greedy with respect to the value function of the original policy, is called ***policy improvement***.  \n",
    "$$ \n",
    "\\begin{split}\n",
    "q_{\\pi}(s, a) & \\doteq E[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=s, A_t=a] \\\\\n",
    "              & = \\sum\\limits_{s',r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')] \n",
    "\\end{split}\n",
    "$$\n",
    "if $q_{\\pi}(s,a)$ is greater than $v_{\\pi}(s)$, it is better to select action $a$ once state $s$ is encountered following policy $\\pi$    \n",
    "Let $π$ and $π'$ be any pair of deterministic policies such that, for all $s ∈ S$  \n",
    "$$ q_{\\pi}(s,\\pi'(s)) \\gt v_{\\pi}(s) $$\n",
    "Then the policy $π'$ must be as good as, or better than, $π$.  \n",
    "And it must have:  \n",
    "$$ v_{\\pi'}(s) \\gt v_{\\pi}(s) $$\n",
    "An original deterministic policy, $π$, and a changed policy, $π'$, that is identical to $π$ except that $π'(s) = a \\neq π(s)$. If $q_{\\pi}(s,a) > v_{\\pi}$, then $\\pi'$ is indeed better than $\\pi$.  \n",
    "$$\n",
    "\\begin{split}\n",
    "v_{\\pi}(s) & \\le q_{\\pi}(s, \\pi'(s)) \\\\\n",
    "           & = E_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=s, A_t=\\pi'(s)]  \\\\\n",
    "           & = E_{\\pi'}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t=s]  \\\\\n",
    "           & \\le E_{\\pi'}[R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, \\pi'(S_{t+1})) | S_t=s]  \\\\\n",
    "           & = E_{\\pi'}[R_{t+1} + \\gamma E_{\\pi'}[R_{t+2} + \\gamma v_{\\pi}(S_{t+2}) | S_{t+1}] | S_t=s] \\\\\n",
    "           & = E_{\\pi'}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 v_{\\pi}(S_{t+2}) | S_t=s]  \\\\\n",
    "           & \\le E_{\\pi'}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + \\dots | S_t=s]  \\\\\n",
    "           & = v_{\\pi'}(s)\n",
    "\\end{split}\n",
    "$$\n",
    "To consider the new *greedy*  policy, $\\pi'$, by:\n",
    "$$ \n",
    "\\begin{split}\n",
    "\\pi'(s) & \\doteq \\arg\\max\\limits_{a}q_{\\pi}(s,a)  \\\\\n",
    "        & = \\arg\\max\\limits_{a}E[R_{t+1} + \\gamma v_{\\pi}(S_{t+1} | S_t=s, A_t=a]  \\\\\n",
    "        & = \\arg\\max\\limits_{a}\\sum\\limits_{s',r}p(s',r|s,a)[r + \\gamma v_{\\pi}(s)] \n",
    "\\end{split}\n",
    "$$\n",
    "If $v_{\\pi'}(s) = v_{\\pi}(s) $, then $v_{\\pi'} = v_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Policy Iteration\n",
    "$$ \\pi_0 \\xrightarrow{E} v_{\\pi_0} \\xrightarrow{I} \\pi_1 \\xrightarrow{E} v_{\\pi_1} \\xrightarrow{I} \\pi_2 \\xrightarrow{E} \\dots \\xrightarrow{I} \\pi_* \\xrightarrow{E} v_{\\pi_*}$$  \n",
    "* ***Initialization***   \n",
    "$ V(s) \\in R $ and $\\pi(s) \\in A(s)$ arbitrarily for all $s \\in S$\n",
    "* ***Policy Evaluation***   \n",
    "Loop:  \n",
    "&emsp;$\\Delta \\leftarrow 0$  \n",
    "&emsp;Loop for each $s \\in S$  \n",
    "&emsp;&emsp;$v \\leftarrow V(s)$  \n",
    "&emsp;&emsp;$V(s) \\leftarrow \\sum\\limits_{s',r}p(s',r|s,\\pi(s))[r + \\gamma v_{\\pi}(s)] $  \n",
    "&emsp;&emsp;$\\Delta \\leftarrow \\max(\\Delta, |v-V(s)|) $  \n",
    "until $\\Delta < \\theta$ (a small positive number determining the accuracy of estimation)\n",
    "* ***Policy Improvement***  \n",
    "*policy-stable* $\\leftarrow$ true   \n",
    "&emsp;Loop for each $s \\in S$  \n",
    "&emsp;&emsp;old-action $\\leftarrow \\pi(s)$  \n",
    "&emsp;&emsp;$\\pi(s) \\leftarrow \\arg \\max_a \\sum\\limits_{s',r}p(s',r|s,\\pi(s))[r + \\gamma v_{\\pi}(s)] $   \n",
    "&emsp;&emsp;If old-action $\\neq \\pi(s)$, then *policy-stable* $\\leftarrow$ *false*    \n",
    "&emsp;If *policy-stable*, then stop and return $V \\approx v_∗$ and $π \\approx π_∗$; else go to 2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 4.4*** The policy iteration algorithm on the previous page has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is ok for pedagogy, but not for actual use. Modify the pseudocode so that convergence is guaranteed.   \n",
    "***Solution:***  The bug can be fixed by adding additional flags. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 4.5*** How would policy iteration be defined for action values?    \n",
    "***Solution:*** \n",
    "* ***Initialization***   \n",
    "$ Q(s,\\pi(s)) \\in R $ and $\\pi(s) \\in A(s)$ arbitrarily for all $s \\in S$\n",
    "* ***Policy Evaluation***   \n",
    "Loop:  \n",
    "&emsp;$\\Delta \\leftarrow 0$  \n",
    "&emsp;Loop for each $s \\in S$  \n",
    "&emsp;&emsp;$q \\leftarrow Q(s)$  \n",
    "&emsp;&emsp;$V(s) \\leftarrow \\sum\\limits_{s',r}p(s',r|s,\\pi(s))[r + \\gamma v_{\\pi}(s)] $  \n",
    "&emsp;&emsp;$\\Delta \\leftarrow \\max(\\Delta, |v-V(s)|) $  \n",
    "until $\\Delta < \\theta$ (a small positive number determining the accuracy of estimation)\n",
    "* ***Policy Improvement***  \n",
    "*policy-stable* $\\leftarrow$ true   \n",
    "&emsp;Loop for each $s \\in S$  \n",
    "&emsp;&emsp;old-action $\\leftarrow \\pi(s)$  \n",
    "&emsp;&emsp;$\\pi(s) \\leftarrow \\arg \\max_a \\sum\\limits_{s',r}p(s',r|s,\\pi(s))[r + \\gamma v_{\\pi}(s)] $   \n",
    "&emsp;&emsp;If old-action $\\neq \\pi(s)$, then *policy-stable* $\\leftarrow$ *false*    \n",
    "&emsp;If *policy-stable*, then stop and return $V \\approx v_∗$ and $π \\approx π_∗$; else go to 2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 4.6***  Suppose you are restricted to considering only policies that are $ε-soft$, meaning that the probability of selecting each action in each state, $s$, is at least $ε/|A(s)|$. Describe qualitatively the changes that would be required in each of the steps 3, 2, and 1, in that order, of the policy iteration algorithm for $v_∗$ (page 80).  \n",
    "***Solution:***  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.2: Jack's Car Rental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Value Iteration\n",
    "$$ \n",
    "\\begin{split}\n",
    "v_{k+1}(s) & \\doteq \\max\\limits_{a}E[R_{t+1} + \\gamma v_k(S_{t+1} | S_t=s, A_t=a] \\\\\n",
    "           & = \\max\\limits_{a}\\sum\\limits_{s',r}p(s',r|s,a)[r + \\gamma v_{\\pi}(s)]\n",
    "\\end{split}\n",
    "$$\n",
    "for all $s \\in S$.  \n",
    "* ***Value Iteration, for estimating $\\pi \\approx \\pi_*$***   \n",
    "Loop:  \n",
    "&emsp;$\\Delta \\leftarrow 0$  \n",
    "&emsp;Loop for each $s \\in S$  \n",
    "&emsp;&emsp;$v \\leftarrow V(s)$  \n",
    "&emsp;&emsp;$V(s) \\leftarrow \\max_a\\sum\\limits_{s',r}p(s',r|s,\\pi(s))[r + \\gamma V_(s')] $  \n",
    "&emsp;&emsp;$\\Delta \\leftarrow \\max(\\Delta, |v-V(s)|) $  \n",
    "until $\\Delta < \\theta$ (a small positive number determining the accuracy of estimation)  \n",
    "Output a deterministic policy $\\pi \\approx \\pi_*$, such that  \n",
    "$ \\pi(s) = \\arg\\max_a \\sum\\limits_{s',r}p(s',r|s,\\pi(s))[r + \\gamma V_(s')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.3: Gambler's Problem\n",
    "***Exercise: 4.8*** Why does the optimal policy for the gambler’s problem have such a curious form? In particular, for capital of 50 it bets it all on one flip, but for capital of 51 it does not. Why is this a good policy?  \n",
    "***Solution:***   \n",
    "[Maybe the best answer.](https://github.com/dennybritz/reinforcement-learning/issues/172#issuecomment-426206163)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Asynchronous Dynamic Programming \n",
    "***Asynchronous DP*** algorithms are in-place iterative DP algorithms that are not orga-\n",
    "nized in terms of systematic sweeps of the state set. Avoiding sweeps does not necessarily mean that we can get away with less computation. It just means that an algorithm does not need to get locked into any hopelessly long sweep before it can make progress improving a policy, so as to improve the algorithm's rate of progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generalized Policy Iteration(GPI)\n",
    "***Generalized policy iteration (GPI)*** to refer to the general idea of letting ***policy evaluation*** and ***policy improvement*** processes interact, independent of the granularity and other details of the two processes.  \n",
    "The value function stabilizes only when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Efficiency of Dynamic Programming\n",
    "**DP** may not be practical for very large problems, but compared with other methods for solving **MDPs**, **DP** methods are actually quite efficient. In practice, **DP** methods can be used with today’s computers to solve **MDPs** with millions of states.   \n",
    "On problems with large state spaces, ***asynchronous DP methods*** are often preferred. Asynchronous methods and other variations of GPI can be applied in such cases and may find good or optimal policies much faster than synchronous methods can.  \n",
    "**Bootstrapping:** Update estimates on the basis of other estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
