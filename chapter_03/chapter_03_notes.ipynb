{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: An Introduction 2nd Edition\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3  -- Finite Markov Decision Processes(MDPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MDPs** are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards.   \n",
    "**MDPs** are a mathematically idealized form of the reinforcement learning problem for which precise theoretical statements can be made. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent–Environment Interface\n",
    "In a *finite* MDP, the sets of states, actions, and rewards ($S$, $A$, and $R$) all have a finite number of elements.   \n",
    "***Four-arguments function for dynamics*** $p : S × R × S × A \\rightarrow [0, 1]$ \n",
    "$$ p(s', r | s, a) \\dot{=} Pr\\{S_t=s', R_t=r | S_{t-1}=s, A_{t-1}\\} $$\n",
    "for all $s', s \\in S$, $r \\in R$, and $a \\in A(s)$. The function $p$ defines the *dynamics* of the MDP.   \n",
    "\n",
    "$$ \\sum\\limits_{s' \\in S} \\sum\\limits_{r \\in R} p(s', r | s, a) = 1 \\hspace{1cm} \\text{for all } s \\in S, a \\in A(s)$$\n",
    "\n",
    "***Three-arguments function for state-transition probabilities*** $p : S × S × A \\rightarrow [0, 1]$ \n",
    "$$ p(s' | s, a) \\dot{=} Pr\\{S_t=s' | S_{t-1}=s, A_{t-1}\\} = \\sum\\limits_{r \\in R} p(s', r | s, a) $$\n",
    "\n",
    "***Two-arguments function for the expected rewards for state-action-next-state triples*** $r : S × A × S\\rightarrow R$ \n",
    "$$ r(s, a) \\dot{=} E[R_t | S_{t-1}=s, A_{t-1}=a] = \\sum\\limits_{r \\in R}r \\sum\\limits_{s' \\in S}p(s', r | s, a) $$\n",
    "\n",
    "***Three-arguments function for the expected rewards for state-action pairs*** $r : S × A \\rightarrow R$ \n",
    "$$ r(s, a, s') \\dot{=} E[R_t | S_{t-1}=s, A_{t-1}=a, S_t=s'] = \\sum\\limits_{r \\in R}r \\frac{p(s', r | s, a)}{p(s' | s, a)} $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* ***Markov property***  \n",
    "The state must include information about all aspects of the past agent–environment interaction that make a difference for the future.  \n",
    "\n",
    "> In general, actions can be any decisions we want to learn how to make, and the states can be anything we can know that might be useful in making them.\n",
    "\n",
    "> The agent–environment boundary represents the limit of the agent’s absolute control, not of its knowledge.\n",
    "\n",
    "The MDP framework is a considerable abstraction of the problem of ***goal-directed*** learning from interaction. It proposes that whatever the details of the sensory, memory, and control apparatus, and whatever objective one is trying to achieve, any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment: \n",
    "* one signal to represent the choices made by the agent (the actions) \n",
    "* one signal to represent the basis on which the choices are made (the states) \n",
    "* one signal to define the agent’s goal (the rewards). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.1*** Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.2*** Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?   \n",
    "***Solution:*** I think the answer is no when the states are infinite like go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.3*** Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could define them farther out—say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in—say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of where to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.4*** Give a table analogous to that in Example 3.3, but for $p(s', r|s, a)$. It should have columns for $s, a, s', r$, and $p(s', r|s, a)$, and a row for every 4-tuple for which $p(s', r|s, a) > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  s  |  a  |  s' |  r  |$p(s', r|s, a)$|\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "|high |search|high|$r_{search}$|$\\alpha$|\n",
    "|high |search|high|$-3$|$(1 - \\alpha)(1 - \\beta)$|\n",
    "|high |search|low |$r_{search}$|$1 - \\alpha$|\n",
    "|low  |search|high|$-3$|$1 - \\beta$|\n",
    "|low  |search|low |$r_{search}$|$\\beta$|\n",
    "|high |wait  |high|$r_{wait}$|$1$|\n",
    "|low  |wait  |low |$r_{wait}$|$1$|\n",
    "|low  |recharge|high|$0$|$1$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Goals and Rewards\n",
    "* ***Reward hypothesis***   \n",
    "That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).  \n",
    "\n",
    "The reward signal is your way of communicating to the robot what you want it to achieve, not how you want it achieved. For example, a chess-playing agent should be rewarded only for actually winning, not for achieving subgoals such as taking its opponent’s pieces or gaining control of the center of the board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Returns and Episodes\n",
    "In general, we seek to maximize the expected return, where the return, denoted Gt, is defined as some specific function of the reward sequence. In the simplest case the return is the sum of the rewards:\n",
    "$$ G_t = R_{t+1} + R_{t+2} + R_{t+3} + \\dots + R_T $$\n",
    "where $T$ is a final time step.  \n",
    "* ***Episode Tasks***  \n",
    "The *agent–environment interaction* breaks naturally into subsequences, which we call *episodes*. Each episode ends in a special state called the *terminal state*, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. In episodic tasks we sometimes need to distinguish the set of all nonterminal states, denoted $S$, from the set of all states plus the terminal state, denoted $S_+$. The time of termination, $T$, is a random variable that normally varies from episode to episode.\n",
    "* ***Continuing Tasks***  \n",
    "The *agent–environment interaction* does not break naturally into identifiable episodes, but goes on continually without limit. The final time step would be $T = \\infty$.  \n",
    "***Discounting Return***  \n",
    "$$ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum\\limits_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "where $\\gamma$ is a parameter, $0 \\le \\gamma \\le 1$, called the *discount rate*.   \n",
    "\n",
    "\n",
    "Returns at successive time steps are related to each other in a way that is important\n",
    "for the theory and algorithms of reinforcement learning:\n",
    "$$\n",
    "\\begin{split}\n",
    "G_t  & = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots  \\\\\n",
    "& = R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\dots) \\\\\n",
    "& = R_{t+1} + \\gamma G_{t+1}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "* ***Notes:***  Return $G_t$ is still finite if the reward is nonzero and constant—if $\\gamma < 1$. For example, if the reward is a constant +1, then the return is\n",
    "$$ G_t = \\sum\\limits_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1-\\gamma}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.6*** Suppose you treated pole-balancing as an *episodic task* but also used discounting, with all rewards zero except for $−1$ upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?  \n",
    "***Solution:*** $G_t = -\\gamma^{T-1}$. The difference is $T$ is a final time step of each episode and vary from episode to episode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.7*** Imagine that you are designing a robot to run a maze. You decide to give it a reward of $+1$ for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?  \n",
    "***Solution:*** Because if you treat it as an episodic task, the return of each time step $G_t$ is always $+1$, there is nothing for agent to maxmize. Besides, the goal is run out the maze as soon as posiable, and there never tell the agent to achieve this goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.8*** Suppose $\\gamma = 0.5$ and the following sequence of rewards is received $R_1 = −1, R_2 = 2, R_3 = 6, R_4 = 3$, and $R_5 = 2$, with $T = 5$. What are $G_0, G_1, \\dots, G_5$? Hint: Work backwards.   \n",
    "***Solution:*** $G_5=0, G_4=R_5=2, G_3=R_4 + \\gamma G_4 = 5, G_2=R_3 + \\gamma G_3=8.5, G_1=R_2 + \\gamma G_2=6.25, G_0=R_1 + \\gamma G_1=2.125$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Unified Notation for Episodic and Continuing Tasks\n",
    "* The return $G_t$:  \n",
    "$$G_t = \\sum\\limits_{k=t+1}^{T}\\gamma^{k-t-1}R_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Policies and Value Functions\n",
    "Formally, a policy is a mapping from states to probabilities of selecting each possible action. If the agent is following policy $π$ at time $t$, then $π(a|s)$ is the probability that $A_t = a$ if $S_t = s$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.11*** If the current state is $S_t$, and actions are selected according to stochastic policy $π$, then what is the expectation of $R_{t+1}$ in terms of $π$ and the four-argument function $p$ (3.2)?  \n",
    "***Solution:*** $$ R_{t+1} = \\sum\\limits_{r \\in R} r \\sum\\limits_{a \\in A(s)} \\pi (a|s) \\sum\\limits_{s' \\in S} p(s', r | s, a)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $v_{\\pi}$ : the ***state-value*** function for policy $\\pi$   \n",
    "$$ \n",
    "v_{\\pi}(s) = E_{\\pi}[G_t|S_t=s] = E_{\\pi}\\big[ \\sum\\limits_{k=0}^{\\infty}\\gamma^k R_{t+k+1} \\big| S_t=s \\big] \n",
    "$$\n",
    "* $q_{\\pi}$ : the ***action-value*** function for policy $\\pi$   \n",
    "$$ \n",
    "q_{\\pi}(s, a) = E_{\\pi}[G_t|S_t=s, A_t=a] = E_{\\pi}\\big[ \\sum\\limits_{k=0}^{\\infty}\\gamma^k R_{t+k+1} \\big| S_t=s, A_t=a \\big] \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any policy $π$ and any state $s$, the following consistency condition holds between the value of $s$ and the value of its possible successor states:  \n",
    "* ***Bellman equation for $v_π$***\n",
    "$$ \n",
    "\\begin{split}\n",
    "v_{\\pi}(s)  & = E_{\\pi}[G_t|S_t=s]  \\\\\n",
    "            & = E_{\\pi}[R_{t+1} + \\gamma G_{t+1}|S_t=s] \\\\\n",
    "            & = E_{\\pi}[R_{t+1}|S_t=s] + E_{\\pi}[\\gamma G_{t+1}|S_t=s] \\\\\n",
    "            & = \\sum\\limits_{a \\in A(s)}\\pi(a|s)\\sum\\limits_{s'}p(s'|s, a)r(s, a, s') + \n",
    "                \\sum\\limits_{a \\in A(s)}\\pi(a|s)\\sum\\limits_{s'}p(s'|s, a)\\gamma E_{\\pi}[G_{t+1}|S_t=s'] \\\\\n",
    "            & = \\sum\\limits_{a \\in A(s)}\\pi(a|s)\\sum\\limits_{s'}p(s'|s, a)\\sum\\limits_{r \\in R}r\\frac{p(s',r|s,a)}{p(s'|s,a)} + \\sum\\limits_{a \\in A(s)}\\pi(a|s)\\sum\\limits_{s'}\\sum\\limits_{r \\in R} p(s', r | s, a)\\gamma E_{\\pi}[G_{t+1}|S_t=s']\\\\\n",
    "            & = \\sum\\limits_{a}\\pi(a|s) \\sum\\limits_{s'}\\sum\\limits_{r}p(s', r|s, a)[r + \\gamma E_{\\pi}[G_{t+1}|S_{t+1}=s']]  \\\\\n",
    "            & = \\sum\\limits_{a}\\pi(a|s) \\sum\\limits_{s', r}p(s', r|s, a)[r + \\gamma v_{\\pi}(s')]\n",
    "\\end{split}\n",
    "$$\n",
    "It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way. \n",
    "* ***Backup Diagarms*** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.12*** The Bellman equation (3.14) must hold for each state for the value function $v_π$ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds for the center state, valued at $+0.7$, with respect to its four neighboring states, valued at $+2.3, +0.4, −0.4,$ and $+0.7$. (These numbers are accurate only to one decimal place.)  \n",
    "***Solution:*** $$ 0.9 * (2.3 + 0.4 + (-0.4) + 0.7) / 4 \\approx 0.7$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A good blog for understanding the Bellman Equation.](https://joshgreaves.com/reinforcement-learning/understanding-rl-the-bellman-equations/)  \n",
    "***Exercise 3.13*** What is the *Bellman equation* for *action-values*, that is, for $q_π$? It must give the action value $q_π(s, a)$ in terms of the action values, $q_π(s', a')$, of possible successors to the *state–action* pair $(s, a)$. Hint: the backup diagram to the right corresponds to this equation. Show the sequence of equations analogous to (3.14), but for action-values.  \n",
    "***Solution:*** \n",
    "* ***Bellman equation for $q_π$***\n",
    "$$ \n",
    "\\begin{split}\n",
    "q_{\\pi}(s,a)  & = E_{\\pi}[G_t|S_t=s, A_t=a]  \\\\\n",
    "            & = E_{\\pi}[R_{t+1} + \\gamma G_{t+1}|S_t=s, A_t=a] \\\\\n",
    "            & = E_{\\pi}[R_{t+1}|S_t=s, A_t=a] + E_{\\pi}[\\gamma G_{t+1}|S_t=s, A_t=a] \\\\\n",
    "            & = r(s,a) + \\sum\\limits_{s'}p(s'|s,a)\\sum\\limits_{a' \\in A(s')}\\pi(a'|s')\\gamma E_{\\pi}[ G_{t+1}|S_t=s', A_t=a'] \\\\\n",
    "            & = \\sum\\limits_{s'}\\sum\\limits_{r}p(s', r|s, a)[r + \\gamma \\sum\\limits_{a' \\in A(s')}\\pi(a'|s')E_{\\pi}[G_{t+1}|S_{t+1}=s', A_t=a']]  \\\\\n",
    "            & = \\sum\\limits_{s', r}p(s', r|s, a)[r + \\gamma \\sum\\limits_{a' \\in A(s')}\\pi(a'|s')q_{\\pi}(s',a')]\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.14*** In the gridworld example, rewards are positive for goals, negative for running into the edge of the world, and zero the rest of the time. Are the signs of these rewards important, or only the intervals between them? Prove, using (3.8), that adding a constant $c$ to all the rewards adds a constant, $v_c$, to the values of all states, and thus does not affect the relative values of any states under any policies. What is $v_c$ in terms of $c$ and $\\gamma$?  \n",
    "***Solution:*** $$ G_t = (R_{t+1}+c) + \\gamma (R_{t+2}+c) + \\gamma^2 (R_{t+3}+c) + \\dots = \\sum\\limits_{k=0}^{\\infty} \\gamma^k R_{t+k+1}+c\\sum\\limits_{k=0}^{\\infty}\\gamma^k $$\n",
    "$$ v_c = c\\sum\\limits_{k=0}^{\\infty}\\gamma^k $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.15*** Give the equation corresponding to this intuition and diagram for the value at the root node, $v_π(s)$, in terms of the value at the expected leaf node, $q_π(s, a)$, given $S_t = s$.  \n",
    "***Solution:*** \n",
    "$$\n",
    "\\begin{split}\n",
    "v_{\\pi}(s)  & = E[q_π(s, a)|S_t=s] \\\\\n",
    "            & = \\sum\\limits_{a}\\pi(a|s) q_π(s, a)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.17*** Give the equation corresponding to this intuition and diagram for the action value, $q_π(s, a)$, in terms of the expected next reward, $R_{t+1}$, and the expected next state value, $v_π(S_{t+1})$, given that $S_t = s$ and $A_t = a$.   \n",
    "***Soution:*** \n",
    "$$\n",
    "\\begin{split}\n",
    "q_π(s, a)   & = E[R + v_π(s')|S_t=s, A_t=a] \\\\\n",
    "            & = \\sum\\limits_{s'}\\sum\\limits_{r}p(s',r|s,a) (r + v_π(s'))\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimal Policies and Optimal Value Functions\n",
    "A policy $π$ is defined to be better than or equal to a policy $π'$ if its expected return is greater than or equal to that of $π'$ for all states. In other words, $π ≥ π'$ if and only if $v_π(s) ≥ v_π'(s)$ for all $s ∈ S$.   \n",
    "Denote all *optimal policies* by $\\pi_*$    \n",
    "Denote all *optimal state-value function* by $v_*$ , $ v_*(s) \\doteq \\max\\limits_{\\pi}v_{\\pi}(s) $ for all $s ∈ S$.   \n",
    "Denote all *optimal action-value function* by $q_*$ , $q_*(s,a) \\doteq q_{\\pi}(s,a)$ for all $s ∈ S$ and $a ∈ A(s)$.   \n",
    "$$ q_*(s,a) = E[R_{t+1} + \\gamma v_*(S_{t+1})|S_t=s, A_t=a] $$\n",
    "Intuitively, the Bellman optimality equation expresses the fact that the value of a state under an optimal policy must equal the expected return for the best action from that state:  \n",
    "* ***The Bellman optimality equation for $v_∗$***\n",
    "$$\n",
    "\\begin{split}\n",
    "v_*(s)  & = \\max\\limits_{a \\in A(s)}q_{\\pi_*}(s, a)    \\\\\n",
    "        & = \\max\\limits_a E_{\\pi_*}[G_t | S_t=s, A_t=a]  \\\\\n",
    "        & = \\max\\limits_a E_{\\pi_*}[R_{t+1} + \\gamma G_{t+1} | S_t=s, A_t=a]  \\\\\n",
    "        & = \\max\\limits_a E_{\\pi_*}[R_{t+1} + \\gamma v_*(S_{t+1}) | S_t=s, A_t=a]  \\\\\n",
    "        & = \\max\\limits_{a}\\sum\\limits_{s',r} p(s',r|s,a)[r + \\gamma v_*(s')]\n",
    "\\end{split}\n",
    "$$\n",
    "* ***The Bellman optimality equation for $q_∗$***  \n",
    "$$\n",
    "\\begin{split}\n",
    "q_∗(s, a) & = E[R_{t+1} + \\gamma \\max\\limits_{a'} q_∗(S_{t+1}, a') | S_t=s, A_t=a] \\\\\n",
    "          & = \\sum\\limits_{s',r} p(s',r|s,a) [r + \\gamma \\max\\limits_{a'} q_∗(s', a')] .\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.20***  \n",
    "***Solution:*** $\\pi_{left}$ if $\\gamma = 0$; $\\pi_{right}$ if $\\gamma = 0.9$ ; Both if $\\gamma = 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.21*** Give the Bellman equation for $q_∗$ for the recycling robot.   \n",
    "***Solution:*** For example:  \n",
    "$$ \n",
    "\\begin{split}\n",
    "q_*(h,s) &= p(h|h,s)(r(h,s,h)+\\gamma \\max\\limits_{a'}q_*(h,a')+p(l|h,s)(r(h,s,l)+\\gamma\\max\\limits_{a'}q_*(l,a')\\\\\n",
    "         &= \\alpha(r_s+\\gamma\\max\\{q_*(h,s), q_*(h,w)\\}+(1-\\alpha)(r_s+\\gamma\\max\\{q_*(l,s),q_*(l,w),q_*(l,r) \\}\n",
    "\\end{split}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.22*** Figure 3.5 gives the optimal value of the best state of the gridworld as $24.4$, to one decimal place. Use your knowledge of the optimal policy and (3.8) to express this value symbolically, and then to compute it to three decimal places.   \n",
    "***Solution:*** \n",
    "$$ \\max\\big\\{22.0*0.9, 22.0*0.9, 16.0*0.9+10\\big\\} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.23*** Give an equation for $v_∗$ in terms of $q_∗$.  \n",
    "***Solution:***  \n",
    "$$ v_*(s) = \\max\\limits_{a}q_*(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.24*** Give an equation for $q_∗$ in terms of $v_∗$ and the world’s dynamics, $p(s', r|s, a)$.  \n",
    "***Solution:***  \n",
    "$$ q_*(s,a) = \\sum\\limits_{s',r} p(s',r|s,a)[r + \\gamma v_*(s')] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.25*** Give an equation for $π_∗$ in terms of $q_∗$.   \n",
    "***Solution:***  \n",
    "$$ \\pi_* = \\arg \\max\\limits_{a}q_*(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.26*** Give an equation for $π_∗$ in terms of $v_∗$ and the world’s dynamics, $p(s', r|s, a)$.   \n",
    "***Solution:*** \n",
    "$$ \\pi_* = \\arg \\max\\limits_{a} \\sum\\limits_{s',r} p(s',r|s,a)[r + \\gamma v_*(s')] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimality and Approximation\n",
    "The **memory available** is also an important constraint. A large amount of memory is often required to build up approximations of value functions, policies, and models. In tasks with small, finite state sets, it is possible to form these approximations using arrays or tables with one entry for each state (or state–action pair). This we call the ***tabular case***, and the corresponding methods we call ***tabular methods***. In many cases of practical interest, however, there are far more states than could possibly be entries in a table. In these cases the functions must be approximated, using some sort of more compact ***parameterized function*** representation.  \n",
    "The on-line nature of reinforcement learning makes it possible to approximate optimal policies in ways that put more effort into learning to make good decisions for frequently encountered states, at the expense of less effort for infrequently encountered states. This is one key property that distinguishes reinforcement learning from other approaches to approximately solving MDPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
